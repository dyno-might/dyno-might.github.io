---
layout: post
title: "Good if make prior after data instead of before"
image: /img/prior/flood.jpg
tags: [math, forecasting]
description: "because truth is many"
excerpt: ""
permalink: /prior/
#seo:
# date_modified: 2025-11-21
# last_modified_at: 2025-11-21
comment:
    lemmy: "https://lemmy.world/post/40386659"
    substack: "https://dynomight.substack.com/p/prior"
head: ""

---

They say you're supposed to choose your prior in advance. That's why it's called a "prior". *First*, you're supposed to say say how plausible different things are, and *then* you update your beliefs based on what you see in the world.

For example, currently you are—I assume—trying to decide if you should stop reading this post and do something else with your life. If you've read this blog before, then lurking somewhere in your mind is some prior for how often my posts are good. For the sake of argument, let's say you think 25% of my posts are funny and insightful and 75% are boring and worthless.

![](/img/prior/example1.svg)

OK. But now here you are reading these words. If they seem bad/good, then that raises the odds that this particular post is worthless/non-worthless. For the sake of argument again, say you find these words mildly promising, meaning that a good post is 1.5× more likely than a worthless post to contain words with this level of quality.

![](/img/prior/example2.svg)

If you combine those two assumptions, that implies that the probability that this particular post is good is 33.3%. That's true because the red rectangle below has half the area of the blue one, and thus the probability that this post is good should be half the probability that it's bad (33.3% vs. 66.6%)

![](/img/prior/example3.svg)

<details markdown="1">
<summary>
(Why half the area? Because the red rectangle is ⅓ as wide and ³⁄₂ as tall as the blue one and ⅓ × ³⁄₂ = ½. If you only trust equations, click here for equations.)
</summary>

It's easiest to calculate the ratio of the odds that the post is good versus bad, namely

```
P[good | words] / P[bad | words]
 = P[good, words] / P[bad, words]
 = (P[good] × P[words | good])
 / (P[bad] × P[words | bad])
 = (0.25 × 1.5) / (0.75 × 1)
 = 0.5
```

It follows that 

```
P[good | words] = 0.5 × P[bad | words],
```
 
and thus that

```
P[good | words] = 1/3.
```

Alternatively, if you insist on using Bayes' equation:

```
P[good | words]
 = P[good] × P[words | good] / P[words]
 = P[good] × P[words | good]
 / (P[good] × P[words | good] + P[bad] × P[words | bad])
 = 0.25 × 1.5 / (0.25 × 1.5 + 0.75)
= (1/3)
```

</details>

Theoretically, when you chose your prior that 25% of dynomight posts are good, that was supposed to reflect all the information you encountered in life *before* reading this post. Changing that number based on information contained in this post wouldn't make any sense, because that information is supposed to be reflected in the second step when you choose your likelihood `p[good | words]`. Changing your prior based on this post would amount to "double-counting".

In theory, that's right. It's also right in practice for the above example, and for the similar [cute little examples](https://en.wikipedia.org/wiki/Representativeness_heuristic#The_taxicab_problem) you find in textbooks.

But for real problems, I've come to believe that refusing to change your prior after you see the data often leads to tragedy. The reason is that in real problems, things are rarely just "good" or "bad", "true" or "false". Instead, truth comes in an infinite number of varieties. And you often can't predict which of these varieties matter until after you've seen the data.

## Aliens

Let me show you what I mean. Say you're wondering if there are aliens on Earth. As far as we know, there's no reason aliens shouldn't have emerged out of the random swirling of molecules on some other planet, developed a technological civilization, built spaceships, and shown up here. So it seems reasonable to choose a prior it's equally plausible that there are aliens or that there are not, i.e. that

```
P[aliens] ≈ P[no aliens] ≈ 50%.
```

![](/img/prior/cartoon2.svg)

Meanwhile, here on our actual world, we have lots of weird alien-esque evidence, like the [Gimbal video](https://en.wikipedia.org/wiki/File:Gimbal_The_First_Official_UAP_Footage_from_the_USG_for_Public_Release.webm), the [Go Fast video](https://en.wikipedia.org/wiki/File:Go_Fast_Official_USG_Footage_of_UAP_for_Public_Release.webm), the [FLIR1 video](https://en.wikipedia.org/wiki/File:FLIR1_Official_UAP_Footage_from_the_USG_for_Public_Release.webm), the [Wow! signal](https://en.wikipedia.org/wiki/Wow!_signal), government reports on [unidentified aerial phenomena](https://en.wikipedia.org/wiki/UFO_Report_(U.S._Intelligence)), and lots of pilots that report seeing "tic-tacs" fly around in physically impossible ways. Call all that stuff `data`. If aliens weren't here, then it seems hard to explain all that stuff. So it seems like `P[data | no aliens]` should be some low number.

On the other hand, if aliens *were* here, then why don't we ever get a good image? Why are there endless confusing reports and rumors and grainy videos, but *never* a single clear close-up high-resolution video, and *never* any alien debris found by some random person on the ground? That also seems hard to explain if aliens *were* here. So I think `P[data | aliens]` should also be some low number. For the sake of simplicity, let's call it a wash and assume that

```
P[data | no aliens] ≈ P[data | aliens].
```

![](/img/prior/cartoon3.svg)


Since neither the prior nor the data see any difference between aliens and no-aliens, the posterior probability is

```
P[no aliens | data] ≈ P[aliens | data] ≈ 50%.
```

![](/img/prior/cartoon4.svg)

See the problem?

<details markdown="1">
<summary>
(Click here for math.)
</summary>

Observe that

```
P[aliens | data] / P[no aliens | data]
 = P[aliens, data] / P[no aliens, data]
 = (P[aliens] × P[data | aliens])
 / (P[no aliens] × P[data | no aliens])
 ≈ 1,
```

where the last line follows from the fact that `P[aliens] ≈ P[no aliens]` and `P[data | aliens] ≈ P[data | no aliens]`. Thus we have that

```
P[aliens | data] ≈ P[no aliens | data] ≈ 50%.
```

</details>

We're friends. We respect each other. So let's not argue about if my starting assumptions are good. They're my assumptions. I like them. And yet the final conclusion seems insane to me. What went wrong?

Assuming I didn't screw up the math (I didn't), the obvious explanation is that I'm experiencing cognitive dissonance as a result of a poor decision on my part to adopt a set of mutually contradictory beliefs. Say you claim that Alice is taller than Bob and Bob is taller than Carlos, but you deny that Alice is taller than Carlos. If so, that would mean that you're confused, not that you've discovered some interesting paradox.

Perhaps if I believe that `P[aliens] ≈ P[no aliens]` and that `P[data | aliens] ≈ P[data | no aliens]`, then I *must* accept that `P[aliens | data] ≈ P[no aliens | data]`. Maybe rejecting that conclusion just means I have some personal issues I need to work on.

I deny that explanation. I deny it! Or, at least, I deny that's it's most helpful way to think about this situation. To see why, let's build a second model.

## More aliens

Here's a trivial observation that turns out to be important: "There are aliens" isn't a single thing. There could be furry aliens, slimy aliens, aliens that like synthwave music, etc. When I stated my prior, I could have given different probabilities to each of those cases. But if I had, it wouldn't have changed anything, because there's no reason to think that furry vs. slimy aliens would have any difference in their eagerness to travel to ape-planets and fly around in physically impossible tic-tacs.

But suppose I had divided up the state of the world into these four possibilities:

| possibility                 | description                                                                                                                                                                                           |
| --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `No aliens + normal people` | There are no aliens. Meanwhile, people are normal and not prone to hallucinating evidence for things that don't exist.                                                                                |
| `No aliens + weird people`  | There are no aliens. Meanwhile, people are weird and *do* tend to hallucinate evidence for things that don't exist.                                                                                   |
| `Normal aliens`             | There are aliens. They may or may not have cool spaceships or enjoy shooting people with lasers. But one way or another, they leave obvious, indisputable evidence that they're around.               |
| `Weird aliens`              | There are aliens. But they stay hidden until humans get interested in space travel. And after that, they let humans take confusing grainy videos, but never a single good video, never ever, not one. |

If I had broken things down that way, I might have chosen this prior:

```
P[no aliens + normal people] ≈ 41%
P[no aliens + weird people] ≈ 9%
P[normal aliens] ≈ 49%
P[weird aliens] ≈ 1%
```

![](/img/prior/cartoon5.svg)

Now, let's think about the empirical evidence again. It's incompatible with `no aliens + normal people`, since if there were no aliens, then normal people wouldn't hallucinate flying tic-tacs. The evidence is *also* incompatible with `normal aliens` since is those kinds of aliens were around they would make their existence obvious. However, the evidence fits pretty well with `weird aliens` and also with `no aliens + weird people`.

So, a reasonable model would be

```
P[data | normal aliens] ≈ 0	
P[data | no aliens + normal people] ≈ 0
P[data | weird aliens] ≈ P[data | no aliens + weird people].
```

![](/img/prior/cartoon6.svg)

If we combine those assumptions, now we only get a 10% posterior probability of aliens.

```
P[no aliens + normal people | data] ≈ 0
P[no aliens + weird people | data] ≈ 90%
P[normal aliens | data] ≈ 0
P[weird aliens | data] ≈ 10%
```

![](/img/prior/cartoon7.svg)

Now the results seem non-insane.

<details markdown="1">
<summary>
(math)
</summary>

To see why, first note that

```
P[normal aliens | data]
 ≈ P[data | no aliens + normal people]
 ≈ 0,
```

since both `normal aliens` and `no aliens + normal people` have near-zero probability of producing the observed data.

Meanwhile,

```
P[no aliens + weird people | data] / P[weird aliens | data]
 = P[no aliens + weird people, data] / P[weird aliens, data]
 ≈ P[no aliens + weird people] / P[weird aliens]
 ≈ .09 / .01
 = 9,
```

where the second equality follows from the fact that the data is assumed to be equally likely under `no aliens + weird people` and `weird people`

It follows that

```
P[no aliens + normal people | data]
 ≈ 9 × P[weird aliens | data],
```

and so

```
P[no aliens + weird people | data] ≈ 90%
P[weird aliens | data] ≈ 10%.
```
	
</details>

## Huh?

I hope you are now confused. If not, let me lay out what's strange: The priors for the two above models *both* say that there's a 50% chance of aliens. The first prior wasn't *wrong*, it was just less detailed than the second one.

That's weird, because the second prior seemed to lead to completely different predictions. If a prior is non-wrong and the math is non-wrong, shouldn't your answers be non-wrong? What the hell?

The simple explanation is that I've been lying to you a little bit. Take any situation where you're trying to determine the truth of anything. Then there's some space of **things that could be true**.

![](/img/prior/cartoon1.svg)

In some cases, this space is finite. If you've got a single tritium atom and you wait a year, either the atom decays or it doesn't. But in most cases, there's a large or infinite space of possibilities. Instead of you just being "sick" or "not sick", you could be "high temperature but in good spirits" or "seems fine except won't stop eating onions".

(Usually the space of things that could be true isn't easy to map to a small 1-D interval. I'm drawing like that for the sake of visualization, but really you should think of it as some high-dimensional space, or even an infinite dimensional space.)

In the case of aliens, the space of things that could be true might include, "There are lots of slimy aliens and a small number of furry aliens and the slimy aliens are really shy and the furry aliens are afraid of squirrels." So, in *principle*, what you should do is divide up the space of things that might be true into tons of extremely detailed things and give a probability to each.

![](/img/prior/cartoon12.svg)

Often, the space of things that could be true is infinite. So theoretically, if you really want to do things by the book, what you should *really* do is specify how plausible each of those (infinite) possibilities is.

After you've done that, you can look at the data. For each thing that could be true, you need to think about the probability of the data. Since there's an infinite number of things that could be true, that's an infinite number of probabilities you need to specify. You could picture it as some curve like this:

![](/img/prior/cartoon8.svg)

(That's a generic curve, not one for aliens.)

To me, this is the most underrated problem with applying Bayesian reasoning to complex real-world situations: In practice, there are an infinite number of things that can be true. It's a lot of work to specify prior probabilities for an infinite number of things. And it's *also* a lot of work to specify the likelihood of your data given an infinite number of things.

So what do we do in practice? We simplify, usually by limiting creating grouping the space of things that could be true into some small number of discrete categories. For the above curve, you might break things down into these four equally-plausible possibilities.

![](/img/prior/cartoon9.svg)

Then you might estimate these data probabilities for each of those possibilities.

![](/img/prior/cartoon10.svg)

Then you could put those together to get this posterior:

![](/img/prior/cartoon11.svg)

That's not bad. *But it is just an approximation*. Your "real" posterior probabilities correspond to these areas:

![](/img/prior/cartoon13.svg)

That approximation was pretty good. But the *reason* it was good is that we started out with a good discretization of the space of things that might be true: One where the likelihood of the data didn't vary too much for the different possibilities inside of `A`, `B`, `C`, and `D`. Imagine the likelihood of the data—if you were able to think about all the infinite possibilities one by one—looked like this:

![](/img/prior/cartoon14.svg)

This is dangerous. The problem is that you can't actually think about all those infinite possibilities. When you think about four four discrete possibilities, you might estimate some likelihood that looks like this:

![](/img/prior/cartoon15.svg)

If you did that, that would lead to you underestimating the probability of `A`, `B`, and `C`, and overestimating the probability of `D`.

This is where my first model of aliens went wrong. My prior `P[aliens]` was not wrong. (Not to me.) The mistake was in assigning the same value to `P[data | aliens]` and `P[data | no aliens]`. Sure, I think the probability of all our alien-esque data is equally likely given aliens and given no-aliens. But that's only true for *certain kinds* of aliens, and *certain kinds* of no-aliens. And my prior for *those kinds* of aliens is much lower than for those kinds of non-aliens.

Technically, the fix to the first model is simple: Make `P[data | aliens]` lower. But the *reason* it's lower is that I have additional prior information that I forgot to include in my original prior. If I just assert that `P[data | aliens]` is much lower than `P[data | no aliens]` then the whole formal Bayesian thing isn't actually doing very much—I might as well just state that I think `P[aliens | data]` is low. If I want to formally justify why `P[data | aliens]` should be lower, that requires a messy recursive procedure where I sort of add that missing prior information and then integrate it out when computing the data likelihood.

<details markdown="1">
<summary>
(math)
</summary>

Mathematically,

```
P[data | aliens]
 = ∫ P[wierd aliens | aliens]
 × P[data | wierd aliens] d(weird aliens)
 + ∫ P[normal aliens | aliens]
 × P[data | normal aliens] d(normal aliens).
```

But now I have to give a detailed prior anyway. So what was the point of starting with a simple one?

</details>

I don't think that technical fix is very good. While it's technically correct (har-har) it's very unintuitive. The better solution is what I did in the second model: To create a finer categorization of the space of things that might be true, such that the probability of the data is constant-ish for each term.

The thing is: Such a categorization depends on the data. Without seeing the actual data in our world, I would never have predicted that we would have so many pilots that report seeing tic-tacs. So I would never have predicted that I should have categories that are based on how much people might hallucinate evidence or how much aliens like to mess with us. So the only practical way to get good results is to *first* look at the data to figure out what categories are important, and *then* to ask yourself how likely you *would* have said those categories were, if you hadn't yet seen any of the evidence.