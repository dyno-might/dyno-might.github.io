---
layout: post
title: "Why I don't believe in long-term thinking"
image: /img/longterm/tree2.jpg
tags: philosophy policy
description: "Do we really know what the future needs?"
permalink: /longterm/
background_color: rgb(211,211,212)
head: "<style>
img{
    display:block;
    margin-left: auto;
    margin-right: auto;
    max-width:min(100%,500pt);
}
details{
    margin-bottom: 10pt;
    background: #eeeeee;
    }
details > summary{
  padding-bottom: 0pt;
  cursor: pointer;
  background: #ffffff;
  padding-bottom: 5pt;
}
details > *:not(summary){
  margin-top: 0pt;
  margin-left: 5pt;
}
table tr{
    border-style: hidden;
    text-align:left;
}
@media (min-width:501px){
table{
  max-width:100;
  max-width:100%;
  font-size: 90%;
}
}
@media (max-width:500px) and (min-width:301px) {
table{
  max-width:100;
  max-width:100%;
  font-size: 2.4vw;
}
}
@media (max-width:300px) {
table{
  max-width:100;
  max-width:100%;
  font-size: 0.5em;
}
.fixed{
    max-width:100;
    max-width:100%;
    overflow:scroll;
}
}
</style>
"
---

*Edit*: This is an April fool's day prank / steelman / venting of my own anxieties, not something I literally believe is true.

## 1.

The argument for long-term thinking goes something like this:

* There are X people alive today.
* In the future, there will be Yâ‰«X people alive.
* All people have equal moral weight.
* Therefore the state of the world in the future is more important than the state of the world today.

Ooookay. Let's assume three things:

1.  We like the above argument.
2.  We can influence the size of the population in the future, through birth control, social subsidies, etc.
3.  We reject the [repugnant conclusion](https://en.wikipedia.org/wiki/Mere_addition_paradox), i.e. we don't think that we're obligated to maximize the human population in the future.

Do you see the problem? We can make the future population smaller, and this is not wrong. Yet, *if* we make the population in the future is smaller, then the state of the world *now* becomes relatively more important. So it's OK to make the future population smaller, and then---because they are smaller---it's OK to give ourselves priority over them. Is there not something very strange about this?

## 2.

Say you're a hunter-gatherer 250,000 years ago, one of the first *homo sapiens* to emerge in the Horn of Africa. Should you live your life in service of the future?

It seems absurd. Even if you accepted that you *should*, how could you predict the future? And if you could predict it, what actions would you take to influence it?

Maybe we have greater leverage today, but are you so sure?

Let's do a thought experiment: Take someone 200 years ago. Let's be very generous and assume they could predict the challenges we face today and derive the optimal actions to help us. Even assuming all that, is there any real difference between *those* actions, and what they would take to make the world better just 180 years ago?

## 3.

There have been past attempts to change society and build a grand future at the cost of some short-term pain. The track record for these attempts is dismal. (Think of the Crusades or the Great Leap Forward.) Throughout history, what has actually worked has been short-term, humble incrementalism. The past teaches us to be suspicious of big ideas, to do the best we can in our own time, not think too far ahead, and to trust in cultural evolution to deliver us.

## 4.

But OK, forget all that. Say long-term thinking is correct. Look around you. What problems do you see in the world?

To me, it's not that people refuse to think long-term. It's that they reject the entire concept of cost-benefit analysis. Either they don't actually care about improving things, or they're so blinded by their value system that they can't see that everything has tradeoffs.

What we need to do is get people on board with the *simplest, most basic* forms of rational thinking. Most now refuse to take a rational approach even towards *current*, *short-term* issues like air pollution, or the threat of everyone dying in a nuclear holocaust *next year*. If someone isn't on-board with those, it's counterproductive to try to convince them to worry about rogue AI or [the suffering of electron orbits](https://reducing-suffering.org/is-there-suffering-in-fundamental-physics/).

The current project is to spread basic, basic, *basic* rational thinking. Even if we assume long-term thinking is right, it's a terrible ambassador for itself. We should focus on charismatic issues that have near term impact, issues someone can align themselves with without looking crazy.

## 5.

Many of my friends have a view of life that's basically: *Productivity! Virtue! Plans! Achievement! Go go go go go!*

This troubles me. I always think:

1. You're an animal, a big drippy sentient meat robot. What would you think about a gorilla that was obsessed with productivity?
2. This view is a way of hiding from your own mortality, to hide from the fact that life has no victory conditions.
3. In the end, we have nothing but our experiences. As you throw yourself at *go go go*, you are missing out on what life is really about.
4. Or are you investing in future experiences? Suppose you become rich/famous/successful. Will you really look back and think it was worth it?
5. Or do you claim your productivity is altruistic? Is it *really*, or are you trying to cheat mortality again?

I admit that for some rare people, *go go go* may be right. But most people who live that way are deluded. The biggest risk is that you miss out on the chance to *live* while you're able to.

Now, apply that logic to our civilization. I dearly hope it thrives and endures until the [Big Freeze/Rip/Crunch/Bounce/Slurp](https://en.wikipedia.org/wiki/Ultimate_fate_of_the_universe). I'll concede that we have a good chance of establishing permanent settlements on other planets. But *self-sustaining* settlements, that could endure without Earth? I'm doubtful.

## 6.

Take the average person 200 years ago. What would they think about us today, with our atheism and homosexuality and inter-racial babies and all the time we spend furiously staring at our glowing rectangles?

What does the future hold? Maybe people will develop morals that seem crazy to us. Maybe people will evolve to discard proxies like love or sex and instead *consciously* [maximize their descendants](https://www.overcomingbias.com/2021/12/on-evolved-values.html). Maybe we'll be uploaded or simulated. Maybe the future doesn't belong to us at all, but to artificial sentient beings that we create.

What will the future want? I don't know, but it seems certain that whoever is around will have different priorities than we do, and it's impossible to predict the change. Even if we *could* impose our values on the future, should we? How can we justify giving such primacy to our own ideas over those of our descendants?


<br>
<br>
So there you go, that's my post for today, April 1, 2022.
