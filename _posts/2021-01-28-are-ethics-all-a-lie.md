---
layout: post
title: "Are ethics all a lie?"
image: /img/ethics/clouds.jpg
tags: philosophy
description: If we only believe in ethics because of evolution, does that mean ethics are a lie?
seo:
  date_modified: 2021-02-16
last_modified_at: 2021-02-16
background_color: rgb(120, 145, 172)
permalink: /:year/:month/:day/:title/
category: "philosophy"
---

Some people claim ethics aren't [practical](https://dynomight.net/2020/10/29/pragmatic-reasons-to-believe-in-formal-ethics/). Others make a grim philosophical argument:

 > Ethics are an illusion. There's no "right and wrong" written in the fabric of the universe. It's all a fiction that exists because it *improves reproductive fitness*. If someone hits you, it feels OK to hit them back. Why? Because if you always get revenge, it's unwise to mess with you. "Morality" is just a hack evolution came up with to win at the remorseless game of reproduction. First this was genetic evolution, now cultural evolution. Either way, beliefs that aid reproduction get transmitted more. Stare reality in the face, and look at ethics for what they are: A thin veneer on top of game theory.

In terms of where our sense of justice comes from, this is correct. Right and wrong don't "really" exist. When we talk about them, we're talking about our *preferences*. We have those preferences because we're programmed to have them. We're programmed that way because they were effective for our (genetic and cultural) ancestors.

Still, does this mean ethics aren't *real*? I don't think so.

Here's what the skeptic is missing: Explaining where our preferences came from doesn't mean those preferences *disappear*.

I want things. I want sentient life to continue to exist. I want people to be happy rather than miserable. I want people to someday figure out the Meaning of Things. Do I want these things because of my human biology and the culture I'm part of? Sure! But still, I *really do* want them.

If you think you're different, consider: Would you hurt a child to make some money? No? What if you could take a pill so you wouldn't *mind* hurting kids. Would you take it?

No. Our ethics might be arbitrary, but we're stuck with them.

Still, what if you *did* rise above your intuitive sense that right and wrong exist. It sounds bad now, but maybe if did it, you'd be happy afterwards?

The problem is, there's nothing up there. Say you eliminate all your arbitrary preferences. Now what? Why get up in the morning? Why do *anything*? Pure cold rationality cannot tell you what to want. We get pleasure/happiness/utility from the "utility function" that's programmed into us. That function might be arbitrary, but the utility is real.

---

Our skeptic might make a second offensive:

> Still, why try to *codify* right and wrong? Our ethical intuitions are a bunch of random hacks and gobbledygook cobbled together by evolution. Why expect these to have a simple core? Why expect them to even be *consistent*?

Again, a lot of this right. Our intuitive ethics are a mess. But our skeptic draws exactly the wrong conclusion from that.

It's precisely *because* our intuitions are so inconsistent that formal ethics are interesting. It's boring to ask *is it wrong to randomly torture people?*
 But it's interesting to ask *is it ever ok to do risky experiments on people to save others?* What if the people are informed volunteers? What if the risks are small? What if the number of saved people is much greater? What if there historical precedents are disturbing? People clearly arrive at [different conclusions](https://en.wikipedia.org/wiki/Human_challenge_study).

That's why ethical riddles like the [trolley problem](https://en.wikipedia.org/wiki/Trolley_problem) are interesting. They clarify ways that our "random hacks" are inconsistent. They are unrealistic scenarios, but that's intentional. Try to have a conversation on the ethics of wealth redistribution or gun ownership or killing [7 billion](https://en.wikipedia.org/wiki/Chick_culling) newly born chickens every year. By removing real-world considerations, we can focus on the contradictions in our programming without getting sidetracked.