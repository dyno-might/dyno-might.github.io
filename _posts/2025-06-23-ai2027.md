---
layout: post
title: "Thoughts on the AI 2027 discourse"
image: /img/ai2027/sliders.jpg
tags: 
description: "what we need is more commentary on the discourse"
excerpt: ""
permalink: /ai2027/
background_color: rgb(82,82,82)
category: "science"
#seo:
#  date_modified: 2025-06-20
#  last_modified_at: 2025-06-20
comment:
  lemmy: "https://old.lemmy.world/post/31869873"
  substack: "https://dynomight.substack.com/p/ai2027"
head: ""

---

A couple of months ago (April 2025), a group of prominent folks released AI 2027, a project that predicted that AGI could plausibly be reached in 2027 and have important consequences. This included a set of [forecasts](https://ai-2027.com/research/) and a [story](https://ai-2027.com/) for how things might play out. This got a lot of attention. Some was positive, some was negative, but it was almost all very high level.

More recently (June 2025) [titotal](https://titotal.substack.com/) released [a detailed critique](https://titotal.substack.com/p/a-deep-critique-of-ai-2027s-bad-timeline), suggesting various flaws in the modeling methodology.

I don't have much to say about AI 2027 or the critique on a technical level. It would take me at least a couple of weeks to produce an opinion worth caring about, and I haven't spent the time. But I would like to comment on the discourse. (Because "What we need is more commentary on the discourse", said no one.)

Very roughly speaking, here's what I remember: First, AI 2027 came out. Everyone cheers. "Yay! Amazing!" Then the critique came out. Everyone boos. "Terrible! AI 2027 is not serious! This is why we need peer-review!"

This makes me feel simultaneously optimistic and depressed.

Should AI 2027 have been peer-reviewed? Well, let me tell you a common story:

1. Someone decides to write a paper.

2. In the hope of getting it accepted to a journal, they write it in arcane academic language, fawningly cite unrelated papers from everyone who could conceivably be a reviewer, and make every possible effort to hide all flaws.

3. This takes 10× longer than it should, results in a paper that's very boring and dense, and makes all limitations illegible.

4. They submit it to a journal.

5. After a long time, some unpaid and distracted peers give the paper a quick once-over and write down some thoughts.

6. There's a cycle where the paper is revised to hopefully make those peers happy. Possibly the paper is terrible, the peers see that, and the paper is rejected. No problem! The authors resubmit it to a different journal.

7. Twelve years later, the paper is published. Oh happy day!

8. You decide to read the paper.

9. After fighting your way through the writing, you find something that seems fishy. But you're not sure, because the paper doesn't fully explain what they did.

10. The paper cites a bunch of other papers in a way that implies they *might* resolve your question. So you read those papers, too. It doesn't help.

11. You look at the supplementary material. It consists of insanely pixelated graphics and tables with labels like `Qetzl_xmpf12` that are never explained.

12. In desperation, you email the authors.

13. They never respond.

14. The end.

And remember, peer review is done by *peers* from the same community who think in similar ways. Different communities settle on somewhat random standards for what's considered important or what's considered an error. In much of the social sciences, for example, quick-and-dirty regressions with strongly implied causality are A+ supergood. Outsiders can complain, but they aren't the ones doing the reviewing.

I wouldn't say that peer review is worthless. It's something! Still, call me cynical—you're not wrong—but I think the number of mistakes in peer-reviewed papers is one to two orders of magnitude higher than generally understood.

Why are there so many mistakes to start with? Well I don't know if you've heard, but humans are fallible creatures. When we build complex things, they tend to be flawed. They *particularly* tend to be flawed when—for example—people have strong incentives to produce a large volume of "surprising" results, and the process to find flaws isn't very rigorous.

Aren't authors motivated by Truth? Otherwise, why choose that life over making lots more money elsewhere? I personally think this is an important factor, and probably the main reason the current system works at all. But still, it's amazing how indifferent many people are to whether their claims are actually correct. They've been in the game so long that all they remember is their [h-index](https://en.wikipedia.org/wiki/H-index).

And what happens if someone spots an error after a paper is published? This happens all the time, but papers are almost never retracted. Nobody wants to make a big deal because, again, *peers*. Why make enemies? Even when publishing a contradictory result later, people tend to word their criticisms so gently and indirectly that they're almost invisible.

As far as I can tell, the main way errors spread is: Gossip. This works sorta-OK-ish for academics, because they *love* gossip and will eagerly spread the flaws of famous papers. But it doesn't happen for obscure papers, and it's invisible to outsiders. And, of course, if seeing the flaws requires new ideas, it won't happen at all.

If peer review is so imperfect, then here's a little dream. Just imagine:

1. Alice develops some ideas and posts them online, quickly and with minimal gatekeeping.

2. Because Alice is a normal human person, there are some mistakes.

3. Bob sees it and thinks something is fishy.

4. Bob asks Alice some questions. Because Alice cares about being right, she's happy to answer those questions.

5. Bob still thinks something is fishy, so he develops a critique and posts it online, quickly and with minimal gatekeeping.

6. Bob's critique is friendly and focuses entirely on technical issues, with no implications of bad faith. But at the same time, he pulls no punches.

7. Because Bob is a normal human person, he makes some mistakes, too.

8. Alice accepts some parts of the critique. She rejects other parts and explains why.

9. Carol and Eve and Frank and Grace see all this and jump in with their own thoughts.

10. Slowly, the collective power of many human brains combine to produce better ideas than any single human could.

Wouldn't that be amazing? And wouldn't it be amazing if some community developed social norms that encouraged people to behave that way? Because as far as I can tell, that's approximately what's happening with AI 2027.

I guess there's a tradeoff in how much you "punish" mistakes. Severe punishment makes people defensive and reduces open discussion. But if you're *too* casual, then people might get sloppy.

My guess is that different situations call for different tradeoffs. Pure math, for example, might do well to set the "punishment slider" fairly high, since verifying proofs is easier than creating the proofs.

The best choice also depends on technology. If it's 1925 and communication is bottlenecked by putting ink on paper, maybe you want to push most of the verification burden onto the original authors. But it's not 1925 anymore, and surely it's time to experiment with new models.